{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with Ensemble Machine Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to ensemble machine learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply speaking, **<font color=cyan>ensemble machine learning refers to a technique that integrates output from multiple learners and is applied to a dataset to make a prediction</font>**. **These multiple learners are usually referred to as base learners**. **When multiple base models are used to extract predictions that are combined into one single prediction, that prediction is likely to provide better accuracy than individual base learners**.\n",
    "\n",
    "**Ensemble models are known for providing an advantage over single models in terms of performance**. They can be applied to **both regression and classification problems**. You can either decide to **build ensemble models with algorithms from the same family or opt to pick them from different families**. **<font color=cyan>If multiple models are built on the same dataset using neural networks only, then that ensemble would be called a homogeneous ensemble model. If multiple models are built using different algorithms, such as support vector machines (SVMs), neural networks, and random forests, then the ensemble model would be called a heterogeneous ensemble model.</font>**\n",
    "\n",
    "The construction of an ensemble model requires two steps:\n",
    "\n",
    "1. **Base learners are learners that are designed and fit on training data**\n",
    "2. The **base learners are combined to form a single prediction model by using specific ensembling techniques** such as *max-voting*, *averaging*, and *weighted averaging*\n",
    "\n",
    "The following diagram shows the structure of the ensemble model:\n",
    "\n",
    "![Alt text](ensemble_ml_diagram.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, to get an ensemble model that performs well, **the base learners themselves should be as accurate as possible**. A common way to **measure the performance of a model** is to **evaluate its generalization error**. A generalization error is a term to **measure how accurately a model is able to make a prediction, based on a new dataset that the model hasn't seen**.\n",
    "\n",
    "To perform well, the ensemble models require a sufficient amount of data. **Ensemble techniques prove to be more useful when you have large and non-linear datasets**.\n",
    "\n",
    "> An ensemble model may overfit if too many models are included, although this isn't very common.\n",
    "\n",
    "Irrespective of how well you fine-tune your models, **there's always the risk of high bias or high variance**. Even the best model can fail if the bias and variance aren't taken into account while training the model. Both bias and variance represent a kind of error in the predictions. In fact, **the total error is comprised of bias-related error, variance-related error, and unavoidable noise-related error (or irreducible error)**. The noise-related error is mainly due to noise in the training data and can't be removed. However, the errors due to bias and variance can be reduced."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total error can be expressed as follows:\n",
    "\n",
    "$\\text{Total Error} = \\text{Bias}^2 + \\text{Variance} + \\text{Irreductible Error}$\n",
    "\n",
    "A measure such as **mean square error (MSE)** captures **all of these errors for a continuous target variable** and can be represented as follows:\n",
    "\n",
    "$$\n",
    "MSE = E[(Y - \\hat{f}(x))^2]\n",
    "$$\n",
    "\n",
    "In this formulat, $E$ stands for **expected mean**, $Y$ represents **the actual target values** and $\\hat{f}(x)$ is the **predicted values for the target variable**. It can be broken down into its components such as bias, variance and noise as shown in the following formula:\n",
    "\n",
    "$$\n",
    "MSE = \\frac{[E[\\hat{f}(x)] - f(x)]^2}{Bias} + \\frac{E[\\hat{f}(x) - E[\\hat{f}(x)]^2]}{Variance} + \\frac{\\epsilon}{Noise}\n",
    "$$\n",
    "\n",
    "While **<font color=cyan>bias refers to how close is the ground truth to the expected value of our estimate</font>**, **<font color=cyan>the variance, on the other hand, measures the deviation from the expected estimator value</font>**. **Estimators with small MSE is what is desirable**. In order to minimize the MSE error, we would like to be centered ($0$-bias) at ground truth and have a low deviation (low variance) from the ground truth (correct) value. In other words, we'd like to be confident (low variance, low uncertainty, more peaked distribution) about the value of our estimate. **High bias degrades the performance of the algorithm on the training dataset and leads to underfitting**. **High variance**, on the other hand, **is characterized by low training errors and high validation errors**. **Having high variance reduces the performance of the learners on unseen data, leading to overfitting**.\n",
    "\n",
    "> Ensemble models can reduce bias and/or variance in the models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max-voting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max-voting, which is **generally used for classification problems**, is one of the simplest ways of combining predictions from multiple machine learning algorithms.\n",
    "\n",
    "**In max-voting, each base model makes a prediction and votes for each sample. Only the sample class with the highest votes is included in the final predictive class**.\n",
    "\n",
    "For example, let's say we have an online survey, in which consumers answer a question in a five-level Likert scale. We can assume that a few consumers will provide a rating of five, while others will provide a rating of four, and so on. If a majority, say more than $50\\%$ of the consumers, provide a rating of four, then the final rating is taken as four. In this example, taking the final rating as four is similar to taking a mode for all of the ratings."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\HP\\\\Documents\\\\Ensemble machine learning\\\\2_getting_started_with_ensemble_ml'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "# set working directory\n",
    "os.chdir(\"../2_getting_started_with_ensemble_ml/\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>Time</th>\n",
       "      <th>Number_of_Warts</th>\n",
       "      <th>Type</th>\n",
       "      <th>Area</th>\n",
       "      <th>Result_of_Treatment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>12.00</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>7.00</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>96</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>11.75</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>750</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>67</td>\n",
       "      <td>9.25</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sex  age   Time  Number_of_Warts  Type  Area  Result_of_Treatment\n",
       "0    1   35  12.00                5     1   100                    0\n",
       "1    1   29   7.00                5     1    96                    1\n",
       "2    1   50   8.00                1     3   132                    0\n",
       "3    1   32  11.75                7     3   750                    0\n",
       "4    1   67   9.25                1     1    42                    0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cryotherapydata = pd.read_csv('./Cryotherapy.csv')\n",
    "df_cryotherapydata.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to do it"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create a voting ensemble model for a classification problem using the `VotingClassifier` class from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier 0.8333333333333334\n",
      "SVC 0.4444444444444444\n",
      "LogisticRegression 0.9444444444444444\n"
     ]
    }
   ],
   "source": [
    "# 1 Import the required libs for building the decision tree, SVM, and logistic regression models\n",
    "# We also import VotingClassifier for max-voting\n",
    "\n",
    "# Import required libs\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# 2 We then move on to the building our feature set and creating our train and test datasets\n",
    "\n",
    "# we create train and test sample from our dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# create feature and response sets\n",
    "feature_columns = ['sex', 'age', 'Time', 'Number_of_Warts', 'Type', 'Area']\n",
    "X = df_cryotherapydata[feature_columns]\n",
    "Y = df_cryotherapydata['Result_of_Treatment']\n",
    "\n",
    "# Create train & test sets\n",
    "X_train, X_test, Y_train, Y_test = \\\n",
    "train_test_split(X, Y, test_size=0.20, random_state=1)\n",
    "\n",
    "# 3 We build our models with the decision tree, SVM, and logistic regression algorithms\n",
    "\n",
    "# create the sub models\n",
    "estimators = []\n",
    "\n",
    "dt_model = DecisionTreeClassifier(random_state=1)\n",
    "estimators.append(('DecisionTree', dt_model))\n",
    "\n",
    "svm_model = SVC(random_state=1)\n",
    "estimators.append(('SupportVector', svm_model))\n",
    "\n",
    "logit_model = LogisticRegression(random_state=1)\n",
    "estimators.append(('Logistic Regression', logit_model))\n",
    "\n",
    "# 4 We build individual models with each of the classifiers we've chosen:\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for each_estimator in (dt_model, svm_model, logit_model):\n",
    "    each_estimator.fit(X_train, Y_train)\n",
    "    Y_pred = each_estimator.predict(X_test)\n",
    "    print(each_estimator.__class__.__name__, accuracy_score(Y_test, Y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Accuracy using Hard Voting:  0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "# 5 We proceed to ensemble our models and use VotingClassifier to score the accuracy\n",
    "# of the ensemble model:\n",
    "\n",
    "# Using VotingClassifier() to build ensemble model with Hard Voting\n",
    "ensemble_model = VotingClassifier(estimators=estimators, voting='hard')\n",
    "\n",
    "ensemble_model.fit(X_train, Y_train)\n",
    "predicted_labels = ensemble_model.predict(X_test)\n",
    "\n",
    "print(\"Classifier Accuracy using Hard Voting: \", accuracy_score(Y_test, predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How it works"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`VotingClassifier` implements two types of voting—**hard** and **soft** voting. In **hard voting, the final class label is predicted as the class label that has been predicted most frequently by the classification models**. In other words, **the predictions from all classifiers are aggregated to predict the class that gets the most votes**. In simple terms, **it takes the mode of the predicted class labels.**\n",
    "\n",
    "In hard voting for the class labels $\\hat{y}$ is the prediction based on the majority voting of each classifier $C_i$, where $i=1 \\dots n$ observations we have the following\n",
    "\n",
    "$$\n",
    "\\hat{y} = mode{C_i(x), C_2(x), \\dots, C_n(x)}\n",
    "$$\n",
    "\n",
    "As shown in the previous section, we have three models, one from the decision tree, one from the SVMs, and one from logistic regression. Let's say that the models classify a training observation as class $1$, class $0$, and class $1$ respectively. Then with majority voting, we have the following:\n",
    "\n",
    "$$\n",
    "\\hat{y} = mode\\{1, 0, 1\\} = 1\n",
    "$$\n",
    "\n",
    "In this case, we would classify the observation as class $1$.\n",
    "\n",
    "In the preceding section, in Step 1, we imported the required libraries to build our models. In Step 2, we created our feature set. We also split our data to create the training and testing samples. In Step 3, we trained three models with the decision tree, SVMs, and logistic regression respectively. In Step 4, we looked at the accuracy score of each of the base learners, while in Step 5, we ensembled the models using `VotingClassifier()` and looked at the accuracy score of the ensemble model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There's more"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many classifiers can estimate class probabilities. In this case, the class labels are predicted by averaging the class probabilities. This is called **soft voting** and is recommended for an ensemble of well-tuned classifiers.\n",
    "\n",
    "In the scikit-learn library, many classification algorithms have the `predict_proba()` method to predict the class probabilities. To perform the ensemble with soft voting, simply replace `voting='hard'` with `voting='soft'` in `VotingClassifier()`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code creates an ensemble using soft voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier 0.8333333333333334\n",
      "SVC 0.4444444444444444\n",
      "LogisticRegression 0.9444444444444444\n",
      "Classifier Accuraty using Soft Voting:  0.8888888888888888\n"
     ]
    }
   ],
   "source": [
    "# create the sub models\n",
    "estimators = []\n",
    "\n",
    "dt_model = DecisionTreeClassifier(random_state=1)\n",
    "estimators.append(('DecisionTree', dt_model))\n",
    "\n",
    "svm_model = SVC(random_state=1, probability=True)\n",
    "estimators.append(('SupportVector', svm_model))\n",
    "\n",
    "logit_model = LogisticRegression(random_state=1)\n",
    "estimators.append(('Logistic Regression', logit_model))\n",
    "\n",
    "for each_estimator in (dt_model, svm_model, logit_model):\n",
    "    each_estimator.fit(X_train, Y_train)\n",
    "    Y_pred = each_estimator.predict(X_test)\n",
    "    print(each_estimator.__class__.__name__, accuracy_score(Y_test, Y_pred))\n",
    "\n",
    "# Using VotingClassifier() to build ensemble model with soft voting\n",
    "ensemble_model = VotingClassifier(estimators=estimators, voting='soft')\n",
    "ensemble_model.fit(X_train, Y_train)\n",
    "predicted_labels = ensemble_model.predict(X_test)\n",
    "print(\"Classifier Accuraty using Soft Voting: \", accuracy_score(Y_test, predicted_labels))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The **SVC class can't estimate class probabilities by default, so we've set its probability hyper-parameter to True** in the preceding code. With `probability=True`, SVC will be able to estimate class probabilities."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Averaging"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Averaging is **usually usec for regression problems** or **can be used while estimating the probabilities in classification tasks**. Predictions are **extracted from multiple models and an average of the predictions are used to make the final prediction**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.7</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.24</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.039</td>\n",
       "      <td>6.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.99340</td>\n",
       "      <td>3.41</td>\n",
       "      <td>0.32</td>\n",
       "      <td>10.400000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.7</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.20</td>\n",
       "      <td>16.00</td>\n",
       "      <td>0.044</td>\n",
       "      <td>41.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>0.99862</td>\n",
       "      <td>3.22</td>\n",
       "      <td>0.46</td>\n",
       "      <td>8.900000</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.26</td>\n",
       "      <td>7.40</td>\n",
       "      <td>0.034</td>\n",
       "      <td>33.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>0.99500</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.42</td>\n",
       "      <td>10.100000</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.3</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1.30</td>\n",
       "      <td>0.036</td>\n",
       "      <td>11.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0.99082</td>\n",
       "      <td>3.48</td>\n",
       "      <td>0.54</td>\n",
       "      <td>11.200000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.4</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.21</td>\n",
       "      <td>9.65</td>\n",
       "      <td>0.041</td>\n",
       "      <td>36.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>0.99334</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.34</td>\n",
       "      <td>10.933333</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            6.7              0.62         0.24            1.10      0.039   \n",
       "1            5.7              0.22         0.20           16.00      0.044   \n",
       "2            5.9              0.19         0.26            7.40      0.034   \n",
       "3            5.3              0.47         0.10            1.30      0.036   \n",
       "4            6.4              0.29         0.21            9.65      0.041   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                  6.0                  62.0  0.99340  3.41       0.32   \n",
       "1                 41.0                 113.0  0.99862  3.22       0.46   \n",
       "2                 33.0                 123.0  0.99500  3.49       0.42   \n",
       "3                 11.0                  74.0  0.99082  3.48       0.54   \n",
       "4                 36.0                 119.0  0.99334  2.99       0.34   \n",
       "\n",
       "     alcohol  quality  \n",
       "0  10.400000        5  \n",
       "1   8.900000        6  \n",
       "2  10.100000        6  \n",
       "3  11.200000        4  \n",
       "4  10.933333        6  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_winedata = pd.read_csv(\"whitewines.csv\")\n",
    "df_winedata.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to do it"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a dataset that is based on the properties of wines. Using this dataset, we'll build multiple regression models with the quality as our response variable. With multiple learners, we extract multiple predictions. The averaging technique would take the average of all of the predicted values for each training sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 import the libraries\n",
    "\n",
    "# Import required libraries\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# 2 create feature and response varable set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# create feature and response variables\n",
    "feature_columns = ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar','chlorides', 'free sulfur dioxide', 'total sulfur dioxide','density', 'pH', 'sulphates', 'alcohol']\n",
    "X = df_winedata[feature_columns]\n",
    "y = df_winedata['quality']\n",
    "\n",
    "# 3 Split the data into training and testing sets\n",
    "# create train & test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=1)\n",
    "\n",
    "# 4 Building the base regression learners using linear regression, SVR and a decision tree:\n",
    "\n",
    "# build base learners\n",
    "linreg_model = LinearRegression()\n",
    "svr_model = SVR()\n",
    "regressiontree_model = DecisionTreeRegressor()\n",
    "\n",
    "# fitting the model\n",
    "linreg_model.fit(X_train, y_train)\n",
    "svr_model.fit(X_train, y_train)\n",
    "regressiontree_model.fit(X_train, y_train)\n",
    "\n",
    "# 5 Use the base learners to make a prediction based on the test data\n",
    "linreg_predictions = linreg_model.predict(X_test)\n",
    "svr_predictions = svr_model.predict(X_test)\n",
    "regtree_predictions = regressiontree_model.predict(X_test)\n",
    "\n",
    "# 6 Add the predictions and divide by the number of base learners\n",
    "# we divide the summation of the predictions by 3 i.e number of base\n",
    "# learners\n",
    "average_predictions=(linreg_predictions + svr_predictions + regtree_predictions)/3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted averaging"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like averaging, weighted averaging is also used for regression tasks. Alternatively, it can be used while estimating probabilities in classification problems. Base learners are assigned different weights, which represent the importance of each model in the prediction.\n",
    "\n",
    "> A weight-averaged model should always be at least as good as your best model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>87139402</td>\n",
       "      <td>B</td>\n",
       "      <td>12.32</td>\n",
       "      <td>12.39</td>\n",
       "      <td>78.85</td>\n",
       "      <td>464.1</td>\n",
       "      <td>0.10280</td>\n",
       "      <td>0.06981</td>\n",
       "      <td>0.03987</td>\n",
       "      <td>0.03700</td>\n",
       "      <td>...</td>\n",
       "      <td>13.50</td>\n",
       "      <td>15.64</td>\n",
       "      <td>86.97</td>\n",
       "      <td>549.1</td>\n",
       "      <td>0.1385</td>\n",
       "      <td>0.1266</td>\n",
       "      <td>0.12420</td>\n",
       "      <td>0.09391</td>\n",
       "      <td>0.2827</td>\n",
       "      <td>0.06771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8910251</td>\n",
       "      <td>B</td>\n",
       "      <td>10.60</td>\n",
       "      <td>18.95</td>\n",
       "      <td>69.28</td>\n",
       "      <td>346.4</td>\n",
       "      <td>0.09688</td>\n",
       "      <td>0.11470</td>\n",
       "      <td>0.06387</td>\n",
       "      <td>0.02642</td>\n",
       "      <td>...</td>\n",
       "      <td>11.88</td>\n",
       "      <td>22.94</td>\n",
       "      <td>78.28</td>\n",
       "      <td>424.8</td>\n",
       "      <td>0.1213</td>\n",
       "      <td>0.2515</td>\n",
       "      <td>0.19160</td>\n",
       "      <td>0.07926</td>\n",
       "      <td>0.2940</td>\n",
       "      <td>0.07587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>905520</td>\n",
       "      <td>B</td>\n",
       "      <td>11.04</td>\n",
       "      <td>16.83</td>\n",
       "      <td>70.92</td>\n",
       "      <td>373.2</td>\n",
       "      <td>0.10770</td>\n",
       "      <td>0.07804</td>\n",
       "      <td>0.03046</td>\n",
       "      <td>0.02480</td>\n",
       "      <td>...</td>\n",
       "      <td>12.41</td>\n",
       "      <td>26.44</td>\n",
       "      <td>79.93</td>\n",
       "      <td>471.4</td>\n",
       "      <td>0.1369</td>\n",
       "      <td>0.1482</td>\n",
       "      <td>0.10670</td>\n",
       "      <td>0.07431</td>\n",
       "      <td>0.2998</td>\n",
       "      <td>0.07881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>868871</td>\n",
       "      <td>B</td>\n",
       "      <td>11.28</td>\n",
       "      <td>13.39</td>\n",
       "      <td>73.00</td>\n",
       "      <td>384.8</td>\n",
       "      <td>0.11640</td>\n",
       "      <td>0.11360</td>\n",
       "      <td>0.04635</td>\n",
       "      <td>0.04796</td>\n",
       "      <td>...</td>\n",
       "      <td>11.92</td>\n",
       "      <td>15.77</td>\n",
       "      <td>76.53</td>\n",
       "      <td>434.0</td>\n",
       "      <td>0.1367</td>\n",
       "      <td>0.1822</td>\n",
       "      <td>0.08669</td>\n",
       "      <td>0.08611</td>\n",
       "      <td>0.2102</td>\n",
       "      <td>0.06784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9012568</td>\n",
       "      <td>B</td>\n",
       "      <td>15.19</td>\n",
       "      <td>13.21</td>\n",
       "      <td>97.65</td>\n",
       "      <td>711.8</td>\n",
       "      <td>0.07963</td>\n",
       "      <td>0.06934</td>\n",
       "      <td>0.03393</td>\n",
       "      <td>0.02657</td>\n",
       "      <td>...</td>\n",
       "      <td>16.20</td>\n",
       "      <td>15.73</td>\n",
       "      <td>104.50</td>\n",
       "      <td>819.1</td>\n",
       "      <td>0.1126</td>\n",
       "      <td>0.1737</td>\n",
       "      <td>0.13620</td>\n",
       "      <td>0.08178</td>\n",
       "      <td>0.2487</td>\n",
       "      <td>0.06766</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0  87139402         B        12.32         12.39           78.85      464.1   \n",
       "1   8910251         B        10.60         18.95           69.28      346.4   \n",
       "2    905520         B        11.04         16.83           70.92      373.2   \n",
       "3    868871         B        11.28         13.39           73.00      384.8   \n",
       "4   9012568         B        15.19         13.21           97.65      711.8   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  points_mean  ...  \\\n",
       "0          0.10280           0.06981         0.03987      0.03700  ...   \n",
       "1          0.09688           0.11470         0.06387      0.02642  ...   \n",
       "2          0.10770           0.07804         0.03046      0.02480  ...   \n",
       "3          0.11640           0.11360         0.04635      0.04796  ...   \n",
       "4          0.07963           0.06934         0.03393      0.02657  ...   \n",
       "\n",
       "   radius_worst  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "0         13.50          15.64            86.97       549.1            0.1385   \n",
       "1         11.88          22.94            78.28       424.8            0.1213   \n",
       "2         12.41          26.44            79.93       471.4            0.1369   \n",
       "3         11.92          15.77            76.53       434.0            0.1367   \n",
       "4         16.20          15.73           104.50       819.1            0.1126   \n",
       "\n",
       "   compactness_worst  concavity_worst  points_worst  symmetry_worst  \\\n",
       "0             0.1266          0.12420       0.09391          0.2827   \n",
       "1             0.2515          0.19160       0.07926          0.2940   \n",
       "2             0.1482          0.10670       0.07431          0.2998   \n",
       "3             0.1822          0.08669       0.08611          0.2102   \n",
       "4             0.1737          0.13620       0.08178          0.2487   \n",
       "\n",
       "   dimension_worst  \n",
       "0          0.06771  \n",
       "1          0.07587  \n",
       "2          0.07881  \n",
       "3          0.06784  \n",
       "4          0.06766  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cancerdata = pd.read_csv(\"wisc_bc_data.csv\")\n",
    "df_cancerdata.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to do it"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we have a dataset based on the properties of cancerous tumors. Using this dataset, we'll **build multiple classification models with diagnosis as our response variable**. The diagnosis variable has the values, `B` and `M`, which indicate **whether the tumor is benign or malignant**. With multiple learners, we extract multiple predictions. The weighted averaging technique takes the average of all of the predicted values for each training sample."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we consider the predicted probabilities as the output and use the `predict_proba()` function of the scikit-learn algorithms to predict the class probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Import the required libraries:\n",
    "\n",
    "# Import required libraries\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# 2 Create the response and feature sets:\n",
    "\n",
    "# Create feature and response variable set\n",
    "# We create train & test sample from our dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# create feature & response variables\n",
    "X = df_cancerdata.iloc[:, 2:32]\n",
    "y = df_cancerdata['diagnosis']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We retrieved the feature columns using the `iloc()` function of the pandas DataFrame, which is purely integer-location based indexing for selection by position. The `iloc()` function takes row and column selection as its parameter, in the form: `data.iloc(<row selection>, <column selection>)`. **The row and column selection can either be an integer list or a slice of rows and columns**. For example, it might look as follows: `df_cancerdata.iloc(2:100, 2:30)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\miniconda3\\envs\\sklearn\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# 3 We'll then split our data into training and testing sets:\n",
    "# Create train & test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1)\n",
    "\n",
    "# 4 Build the base classifier models:\n",
    "# create the sub models\n",
    "estimators = []\n",
    "\n",
    "dt_model = DecisionTreeClassifier()\n",
    "estimators.append(('DecisionTree', dt_model))\n",
    "\n",
    "svm_model = SVC(probability=True)\n",
    "estimators.append(('SupportVector', svm_model))\n",
    "\n",
    "logit_model = LogisticRegression()\n",
    "estimators.append(('Logistic Regression', logit_model))\n",
    "\n",
    "#  5 Fit the models on the test data:\n",
    "dt_model.fit(X_train, y_train)\n",
    "svm_model.fit(X_train, y_train)\n",
    "logit_model.fit(X_train, y_train)\n",
    "\n",
    "# 6 Use the predict_proba() function to predict the class probabilities:\n",
    "dt_predictions = dt_model.predict_proba(X_test)\n",
    "svm_predictions = svm_model.predict_proba(X_test)\n",
    "logit_predictions = logit_model.predict_proba(X_test)\n",
    "\n",
    "# 7 Assign different weights to each of the models to get our final predictions:\n",
    "weighted_average_predictions = (dt_predictions * 0.3 + svm_predictions * 0.4 + logit_predictions * 0.3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "abba74b63ac07f00554c7676ecb12976ca4959712221a1ddcaf32d27fe78653e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
